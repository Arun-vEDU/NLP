{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter\n",
    "from itertools import combinations_with_replacement\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK Reuters Dataset\n",
    "import nltk\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reuters corpus\n",
    "categories = reuters.categories()\n",
    "corpus = [reuters.words(fileid) for fileid in reuters.fileids(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess corpus: Lowercase and split sentences\n",
    "corpus = [[word.lower() for word in sentence] for sentence in corpus]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary and numericalization\n",
    "vocab = list(set(flatten(corpus)))\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "vocab.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocab) - 1\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "voc_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a frequency threshold\n",
    "min_word_freq = 100\n",
    "filtered_vocab = [word for word, count in Counter(flatten(corpus)).items() if count >= min_word_freq]\n",
    "word2index = {w: i for i, w in enumerate(filtered_vocab)}\n",
    "filtered_vocab.append('<UNK>')\n",
    "word2index['<UNK>'] = len(filtered_vocab) - 1\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "voc_size = len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically modify window size\n",
    "def set_window_size(window_size=2):\n",
    "    skip_grams = []\n",
    "    for sent in corpus:\n",
    "        for i, target in enumerate(sent):\n",
    "            context_indices = range(max(0, i - window_size), min(len(sent), i + window_size + 1))\n",
    "            for j in context_indices:\n",
    "                if i != j:\n",
    "                    skip_grams.append((target, sent[j]))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default window size\n",
    "skip_grams = set_window_size(window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence Matrix and Weighting Function\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1\n",
    "    x_max = 5\n",
    "    alpha = 0.75\n",
    "    return (x_ij / x_max) ** alpha if x_ij < x_max else 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     X_ik[(bigram[\u001b[38;5;241m1\u001b[39m], bigram[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;241m=\u001b[39m co_occur \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m weighting_dic[bigram] \u001b[38;5;241m=\u001b[39m weighting(bigram[\u001b[38;5;241m0\u001b[39m], bigram[\u001b[38;5;241m1\u001b[39m], X_ik)\n\u001b[1;32m---> 11\u001b[0m weighting_dic[(bigram[\u001b[38;5;241m1\u001b[39m], bigram[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;241m=\u001b[39m weighting(bigram[\u001b[38;5;241m1\u001b[39m], bigram[\u001b[38;5;241m0\u001b[39m], X_ik)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "X_ik = {}\n",
    "weighting_dic = {}\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        co_occur = X_ik_skipgram[bigram]\n",
    "        X_ik[bigram] = co_occur + 1\n",
    "        X_ik[(bigram[1], bigram[0])] = co_occur + 1\n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Batch Generator\n",
    "def random_batch(batch_size, skip_grams, X_ik, weighting_dic):\n",
    "    skip_grams_id = [(word2index.get(w1, word2index['<UNK>']), word2index.get(w2, word2index['<UNK>'])) for w1, w2 in skip_grams]\n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        pair = skip_grams[i]\n",
    "        cooc = X_ik.get(pair, 1)\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        weighting = weighting_dic.get(pair, 0.0)\n",
    "        random_weightings.append([weighting])\n",
    "    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Model Class\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words)\n",
    "        target_embeds = self.embedding_u(target_words)\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/1000 | Loss: 11.1475 | Time: 0m 1.0000s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     16\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 17\u001b[0m     input_batch, target_batch, cooc_batch, weighting_batch \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_grams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ik\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighting_dic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     input_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(input_batch)\n\u001b[0;32m     19\u001b[0m     target_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(target_batch)\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mrandom_batch\u001b[1;34m(batch_size, skip_grams, X_ik, weighting_dic)\u001b[0m\n\u001b[0;32m      3\u001b[0m skip_grams_id \u001b[38;5;241m=\u001b[39m [(word2index\u001b[38;5;241m.\u001b[39mget(w1, word2index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]), word2index\u001b[38;5;241m.\u001b[39mget(w2, word2index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m w1, w2 \u001b[38;5;129;01min\u001b[39;00m skip_grams]\n\u001b[0;32m      4\u001b[0m random_inputs, random_labels, random_coocs, random_weightings \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m----> 5\u001b[0m random_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskip_grams_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m random_index:\n\u001b[0;32m      8\u001b[0m     random_inputs\u001b[38;5;241m.\u001b[39mappend([skip_grams_id[i][\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:982\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Arunya Senadeera\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2979\u001b[0m, in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2974\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2975\u001b[0m                      initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m-> 2979\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[0;32m   2980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2981\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2982\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2983\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[0;32m   2984\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3098\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mmultiply, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out,\n\u001b[0;32m   3101\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "batch_size = 100\n",
    "embedding_size = 50\n",
    "model = GloVe(voc_size, embedding_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    cooc_batch = torch.FloatTensor(cooc_batch)\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs} | Loss: {loss.item():.4f} | Time: {epoch_mins}m {epoch_secs:.4f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation\n",
    "print(\"Dataset Source: NLTK Reuters Corpus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
