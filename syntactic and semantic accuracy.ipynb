{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the reuters dataset if not already present\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus from the NLTK Reuters dataset\n",
    "def preprocess_reuters():\n",
    "    \"\"\"\n",
    "    Preprocess the Reuters dataset into tokenized sentences for training GloVe.\n",
    "    Returns:\n",
    "        sentences (list): A list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = nltk.word_tokenize(reuters.raw(file_id).lower())\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train GloVe using Word2Vec (CBOW-based approximation)\n",
    "def train_glove_model(sentences, vector_size=100, window_size=2, min_count=5, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a GloVe-like model using Word2Vec from Gensim.\n",
    "    Parameters:\n",
    "        sentences (list): Tokenized sentences from the corpus.\n",
    "        vector_size (int): Dimensionality of the word embeddings.\n",
    "        window_size (int): The context window size (default: 2).\n",
    "        min_count (int): Minimum word frequency to include in the vocabulary.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        model (Word2Vec): The trained Word2Vec model.\n",
    "        training_loss (list): List of losses after each epoch.\n",
    "        training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        vector_size=vector_size,\n",
    "        window=window_size,\n",
    "        min_count=min_count,\n",
    "        sg=0,  # CBOW model (default behavior of GloVe)\n",
    "        compute_loss=True  # Enable loss computation\n",
    "    )\n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "        training_loss.append(loss)\n",
    "\n",
    "    # Record the end time\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return model, training_loss, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Reuters dataset\n",
    "sentences = preprocess_reuters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/10...\n",
      "Epoch 1, Loss: 0.0\n",
      "Training epoch 2/10...\n",
      "Epoch 2, Loss: 0.0\n",
      "Training epoch 3/10...\n",
      "Epoch 3, Loss: 0.0\n",
      "Training epoch 4/10...\n",
      "Epoch 4, Loss: 0.0\n",
      "Training epoch 5/10...\n",
      "Epoch 5, Loss: 0.0\n",
      "Training epoch 6/10...\n",
      "Epoch 6, Loss: 0.0\n",
      "Training epoch 7/10...\n",
      "Epoch 7, Loss: 0.0\n",
      "Training epoch 8/10...\n",
      "Epoch 8, Loss: 0.0\n",
      "Training epoch 9/10...\n",
      "Epoch 9, Loss: 0.0\n",
      "Training epoch 10/10...\n",
      "Epoch 10, Loss: 0.0\n",
      "Total Training Time: 5.85 seconds\n",
      "Training Loss Per Epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Train the GloVe-like model\n",
    "glove_model, training_loss, training_time = train_glove_model(sentences, vector_size=100, window_size=2)\n",
    "\n",
    "# Print training summary\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Training Loss Per Epoch: {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 nouns in the corpus:\n",
      "[('The', 10967), ('U.S.', 4919), ('Net', 3378), ('Shr', 3201), ('Inc', 2663), ('Corp', 2336), ('Revs', 2283), ('It', 2059), ('April', 1902), ('March', 1817), ('Bank', 1704), ('He', 1613), ('February', 1574), ('Japan', 1555), ('January', 1549), ('In', 1474), ('Co', 1387), ('A', 1276), ('Ltd', 1179), ('But', 1157), ('Avg', 1062), ('Oper', 1048), ('May', 964), ('Japanese', 890), ('December', 881), ('Sales', 873), ('West', 858), ('United', 835), ('International', 822), ('New', 795), ('American', 792), ('I', 703), ('We', 688), ('June', 670), ('Group', 653), ('States', 640), ('European', 625), ('Commission', 625), ('Department', 622), ('Year', 620), ('This', 593), ('U.S', 579), ('They', 562), ('Federal', 555), ('National', 549), ('Canada', 546), ('Canadian', 537), ('Nine', 527), ('Minister', 525), ('U.K.', 522)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Extract and preprocess the corpus\n",
    "def get_corpus():\n",
    "    \"\"\"\n",
    "    Extract tokenized words from the Reuters corpus and preprocess them.\n",
    "    Returns:\n",
    "        tokenized_words (list): List of all words in the corpus.\n",
    "    \"\"\"\n",
    "    tokenized_words = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = word_tokenize(reuters.raw(file_id))\n",
    "        tokenized_words.extend(words)\n",
    "    return tokenized_words\n",
    "\n",
    "# Identify nouns in the corpus\n",
    "def identify_nouns(words):\n",
    "    \"\"\"\n",
    "    Identify proper nouns and common nouns from the word list.\n",
    "    Parameters:\n",
    "        words (list): List of words.\n",
    "    Returns:\n",
    "        nouns (list): List of potential nouns (words starting with uppercase).\n",
    "    \"\"\"\n",
    "    nouns = [word for word in words if word.istitle()]  # Filter capitalized words\n",
    "    return nouns\n",
    "\n",
    "# Get the corpus\n",
    "corpus_words = get_corpus()\n",
    "\n",
    "# Count and print a sample of nouns\n",
    "nouns = identify_nouns(corpus_words)\n",
    "nouns_count = Counter(nouns)\n",
    "\n",
    "# Print the top 50 most common nouns\n",
    "print(\"Top 50 nouns in the corpus:\")\n",
    "print(nouns_count.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'Canada': [ 0.4262926  -1.2423606  -0.61886513  0.23888387 -0.0798335   0.6545212\n",
      "  1.1627374   0.25314656  0.21703158 -1.6847712  -0.8597218  -1.535619\n",
      "  1.060118    0.26586792 -0.15536933  0.40483266  0.29103097  0.05220363\n",
      "  0.3548324  -0.14567353  0.6206696  -1.0312492   0.04680159  0.98125803\n",
      "  1.1642821  -0.13911815 -1.1806442  -0.22721769  0.9707035  -0.72080296\n",
      "  0.00381474 -0.18983811  0.5630053   0.00452598 -0.18025659 -0.6592921\n",
      "  0.52549076  0.20244564 -0.36795744  0.6444692   0.8270927   0.22629467\n",
      " -0.9919469  -0.02519848 -0.06340182  0.7273546  -0.10364688 -0.00286774\n",
      "  0.49039426  0.58488     0.09937352 -1.5187933   0.38697448 -0.7598813\n",
      " -1.2156956  -0.14334778  0.11882898  0.36145663  1.4550567   1.4228486\n",
      " -0.01983459 -0.98798203  0.07294873 -0.9175459  -0.5089837   1.469861\n",
      " -0.86075413 -0.34533402 -1.4331465  -0.31300193  0.05041151 -0.6796521\n",
      " -0.19805384 -1.0174718   1.0101024  -0.23672527  0.39137456 -0.02432116\n",
      " -0.8364139  -0.45259434 -0.89181364  1.1022284  -0.69759    -0.58329517\n",
      " -0.34543297  0.6994001   1.0640031  -1.0172639   0.19584171 -0.76223797\n",
      "  1.7022033  -0.47153062 -0.63193846  0.1785024  -0.28795648  0.17927893\n",
      "  0.24727787  0.8354873  -0.14454919  0.23510395]\n",
      "Most similar to 'Minister': [('yasuhiro', 0.7145836353302002), ('commissioner', 0.68733149766922), ('sheikh', 0.6832748055458069), ('margaret', 0.6788630485534668), ('undersecretary', 0.6567996144294739), ('kiichi', 0.6564756631851196), ('manuel', 0.6490585207939148), ('edouard', 0.6474553346633911), ('hernandez', 0.639258623123169), ('kanon', 0.6346323490142822)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the trained model\n",
    "word_vectors = glove_model.wv # holds the trained word vectors (embeddings) learned by the Word2Vec model\n",
    "print(\"Vector for 'Canada':\", word_vectors['canada'])\n",
    "print(\"Most similar to 'Minister':\", word_vectors.most_similar('minister'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the word_vectors from  trained model\n",
    "word_vectors = glove_model.wv  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "# Load the word analogy dataset\n",
    "def download_analogy_dataset():\n",
    "    url = \"https://www.fit.vut.cz/person/imikolov/public/rnnlm/word-test.v1.txt\"\n",
    "    file_name = \"word-test.v1.txt\"\n",
    "    if not os.path.exists(file_name):\n",
    "        response = requests.get(url)\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    return file_name\n",
    "\n",
    "# Download and save the word analogy dataset\n",
    "file_path = download_analogy_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the word analogy dataset\n",
    "def load_word_analogies(filepath):\n",
    "    categories = {}\n",
    "    current_category = None\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\":\"):  # Indicates a new category\n",
    "                current_category = line[1:].strip()\n",
    "                categories[current_category] = []\n",
    "            elif current_category is not None:\n",
    "                categories[current_category].append(line.split())\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Load the word analogies dataset\n",
    "word_analogies = load_word_analogies(\"word-test.v1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available categories: dict_keys(['capital-common-countries', 'capital-world', 'currency', 'city-in-state', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram4-superlative', 'gram5-present-participle', 'gram6-nationality-adjective', 'gram7-past-tense', 'gram8-plural', 'gram9-plural-verbs'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a function to evaluate a specific category\n",
    "def evaluate_category(category, word_analogies, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for analogy in word_analogies[category]:\n",
    "        if all(word in model.key_to_index for word in analogy):\n",
    "            predicted_word, _ = model.most_similar(\n",
    "                positive=[analogy[1], analogy[2]], negative=[analogy[0]], topn=1\n",
    "            )[0]\n",
    "            if predicted_word == analogy[3]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    return correct, total, accuracy\n",
    "\n",
    "# Check available categories in the dataset\n",
    "print(\"Available categories:\", word_analogies.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy (capital-common-countries): 0.00%\n",
      "  Correct: 0, Total: 0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate semantic category\n",
    "semantic_category = \"capital-common-countries\"\n",
    "if semantic_category in word_analogies:\n",
    "    semantic_correct, semantic_total, semantic_accuracy = evaluate_category(\n",
    "        semantic_category, word_analogies, word_vectors\n",
    "    )\n",
    "    print(f\"Semantic Accuracy ({semantic_category}): {semantic_accuracy:.2f}%\")\n",
    "    print(f\"  Correct: {semantic_correct}, Total: {semantic_total}\")\n",
    "else:\n",
    "    print(f\"Category '{semantic_category}' not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'past-tense' not found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate syntactic (past-tense) categories\n",
    "\n",
    "syntactic_category = \"past-tense\"\n",
    "\n",
    "# Evaluate syntactic category\n",
    "if syntactic_category in word_analogies:\n",
    "    syntactic_correct, syntactic_total, syntactic_accuracy = evaluate_category(\n",
    "        syntactic_category, word_analogies, word_vectors\n",
    "    )\n",
    "    print(f\"Syntactic Accuracy ({syntactic_category}): {syntactic_accuracy:.2f}%\")\n",
    "    print(f\"  Correct: {syntactic_correct}, Total: {syntactic_total}\")\n",
    "else:\n",
    "    print(f\"Category '{syntactic_category}' not found in the dataset.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
