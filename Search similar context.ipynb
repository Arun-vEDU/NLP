{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the reuters dataset if not already present\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus from the NLTK Reuters dataset\n",
    "def preprocess_reuters():\n",
    "    \"\"\"\n",
    "    Preprocess the Reuters dataset into tokenized sentences for training GloVe.\n",
    "    Returns:\n",
    "        sentences (list): A list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = nltk.word_tokenize(reuters.raw(file_id).lower())\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train GloVe using Word2Vec (CBOW-based approximation)\n",
    "def train_glove_model(sentences, vector_size=100, window_size=2, min_count=5, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a GloVe-like model using Word2Vec from Gensim.\n",
    "    Parameters:\n",
    "        sentences (list): Tokenized sentences from the corpus.\n",
    "        vector_size (int): Dimensionality of the word embeddings.\n",
    "        window_size (int): The context window size (default: 2).\n",
    "        min_count (int): Minimum word frequency to include in the vocabulary.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        model (Word2Vec): The trained Word2Vec model.\n",
    "        training_loss (list): List of losses after each epoch.\n",
    "        training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        vector_size=vector_size,\n",
    "        window=window_size,\n",
    "        min_count=min_count,\n",
    "        sg=0,  # CBOW model (default behavior of GloVe)\n",
    "        compute_loss=True  # Enable loss computation\n",
    "    )\n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "        training_loss.append(loss)\n",
    "\n",
    "    # Record the end time\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return model, training_loss, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Reuters dataset\n",
    "sentences = preprocess_reuters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/10...\n",
      "Epoch 1, Loss: 0.0\n",
      "Training epoch 2/10...\n",
      "Epoch 2, Loss: 0.0\n",
      "Training epoch 3/10...\n",
      "Epoch 3, Loss: 0.0\n",
      "Training epoch 4/10...\n",
      "Epoch 4, Loss: 0.0\n",
      "Training epoch 5/10...\n",
      "Epoch 5, Loss: 0.0\n",
      "Training epoch 6/10...\n",
      "Epoch 6, Loss: 0.0\n",
      "Training epoch 7/10...\n",
      "Epoch 7, Loss: 0.0\n",
      "Training epoch 8/10...\n",
      "Epoch 8, Loss: 0.0\n",
      "Training epoch 9/10...\n",
      "Epoch 9, Loss: 0.0\n",
      "Training epoch 10/10...\n",
      "Epoch 10, Loss: 0.0\n",
      "Total Training Time: 5.22 seconds\n",
      "Training Loss Per Epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Train the GloVe-like model\n",
    "glove_model, training_loss, training_time = train_glove_model(sentences, vector_size=100, window_size=2)\n",
    "\n",
    "# Print training summary\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Training Loss Per Epoch: {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 nouns in the corpus:\n",
      "[('The', 10967), ('U.S.', 4919), ('Net', 3378), ('Shr', 3201), ('Inc', 2663), ('Corp', 2336), ('Revs', 2283), ('It', 2059), ('April', 1902), ('March', 1817), ('Bank', 1704), ('He', 1613), ('February', 1574), ('Japan', 1555), ('January', 1549), ('In', 1474), ('Co', 1387), ('A', 1276), ('Ltd', 1179), ('But', 1157), ('Avg', 1062), ('Oper', 1048), ('May', 964), ('Japanese', 890), ('December', 881), ('Sales', 873), ('West', 858), ('United', 835), ('International', 822), ('New', 795), ('American', 792), ('I', 703), ('We', 688), ('June', 670), ('Group', 653), ('States', 640), ('European', 625), ('Commission', 625), ('Department', 622), ('Year', 620), ('This', 593), ('U.S', 579), ('They', 562), ('Federal', 555), ('National', 549), ('Canada', 546), ('Canadian', 537), ('Nine', 527), ('Minister', 525), ('U.K.', 522)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Extract and preprocess the corpus\n",
    "def get_corpus():\n",
    "    \"\"\"\n",
    "    Extract tokenized words from the Reuters corpus and preprocess them.\n",
    "    Returns:\n",
    "        tokenized_words (list): List of all words in the corpus.\n",
    "    \"\"\"\n",
    "    tokenized_words = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = word_tokenize(reuters.raw(file_id))\n",
    "        tokenized_words.extend(words)\n",
    "    return tokenized_words\n",
    "\n",
    "# Identify nouns in the corpus\n",
    "def identify_nouns(words):\n",
    "    \"\"\"\n",
    "    Identify proper nouns and common nouns from the word list.\n",
    "    Parameters:\n",
    "        words (list): List of words.\n",
    "    Returns:\n",
    "        nouns (list): List of potential nouns (words starting with uppercase).\n",
    "    \"\"\"\n",
    "    nouns = [word for word in words if word.istitle()]  # Filter capitalized words\n",
    "    return nouns\n",
    "\n",
    "# Get the corpus\n",
    "corpus_words = get_corpus()\n",
    "\n",
    "# Count and print a sample of nouns\n",
    "nouns = identify_nouns(corpus_words)\n",
    "nouns_count = Counter(nouns)\n",
    "\n",
    "# Print the top 50 most common nouns\n",
    "print(\"Top 50 nouns in the corpus:\")\n",
    "print(nouns_count.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'Canada': [-0.03537919 -1.2833431  -0.66349804  0.05484341 -0.7680373   0.2868829\n",
      "  0.67401016 -0.28792086 -0.04003252 -1.3796362  -0.6930536  -1.8006237\n",
      "  0.43856108  0.23824672  0.55158156  0.5020647   0.04388599 -0.8035705\n",
      "  0.01951286 -0.35358974  0.2619007  -1.4447633  -0.03323268  0.62672305\n",
      "  1.0495601  -0.72986037 -1.3692279  -0.0326714   0.83780324 -1.0919042\n",
      "  0.29879636 -0.64934504  0.553407    0.09065519 -0.17620015 -0.3332687\n",
      "  0.23639782 -0.2513296  -0.22948718  0.49308717  0.6384657  -0.3996525\n",
      " -1.1350772   0.51865417  0.14118731  0.29117116  0.13113068  0.00982796\n",
      "  0.98942065  1.4036468   0.3824025  -0.45662957  0.05803205 -1.208544\n",
      " -1.224156   -0.41742542  0.12344661  0.30824795  1.0621655   1.8378152\n",
      "  0.30115926 -1.1696117   0.206364   -0.9058469  -0.00688341  1.4720991\n",
      " -1.3681623   0.16797489 -0.4153084  -0.44217718 -0.1435973  -0.90141994\n",
      "  0.5362748  -0.6637763   0.8872234   0.6649782   0.47312355  0.5114587\n",
      " -0.9459733  -1.0157849  -0.96248007  1.3460357  -0.2749347   0.07711433\n",
      " -0.4278531   0.4139032   1.1508574  -0.81237763 -0.7206817  -0.72174317\n",
      "  1.45694     0.41243514 -0.22520617 -0.06406672  0.01629645 -0.09062888\n",
      "  0.3248695   0.9560781   0.17357184  0.66038585]\n",
      "Most similar to 'Minister': [('margaret', 0.6976727843284607), ('yasuhiro', 0.6930541396141052), ('sheikh', 0.6747853755950928), ('undersecretary', 0.6684812903404236), ('kiichi', 0.6502513289451599), ('edouard', 0.6484083533287048), ('commissioner', 0.6419044733047485), ('grisanti', 0.6286579966545105), ('gerhard', 0.6269470453262329), ('secretary', 0.615522027015686)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the trained model\n",
    "word_vectors = glove_model.wv # holds the trained word vectors (embeddings) learned by the Word2Vec model\n",
    "print(\"Vector for 'Canada':\", word_vectors['canada'])\n",
    "print(\"Most similar to 'Minister':\", word_vectors.most_similar('minister'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the word_vectors from  trained model\n",
    "word_vectors = glove_model.wv  \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "# Save the word vectors\n",
    "word_vectors.save(\"word_vectors.kv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dot_product(query, model, top_n=10):\n",
    "    \"\"\"\n",
    "    Compute the dot product between the input query and all words in the corpus\n",
    "    and retrieve the top_n most similar contexts.\n",
    "    \"\"\"\n",
    "    # Check if the input is a word or a vector\n",
    "    if isinstance(query, str):\n",
    "        # Convert word to vector\n",
    "        query_vector = model[query]\n",
    "    else:\n",
    "        # If query is already a vector, use it directly\n",
    "        query_vector = query\n",
    "\n",
    "    # Compute dot product for all vectors in the corpus\n",
    "    similar_words = []\n",
    "    for word in model.index_to_key:  # Iterate over all words in the vocabulary\n",
    "        word_vector = model[word]\n",
    "        dot_product = np.dot(query_vector, word_vector)\n",
    "        similar_words.append((word, dot_product))\n",
    "\n",
    "    # Sort by dot product in descending order and return the top_n results\n",
    "    similar_words = sorted(similar_words, key=lambda x: x[1], reverse=True)\n",
    "    return similar_words[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words to 'secretary':\n",
      "secretary: 120.36280822753906\n",
      "minister: 95.57017517089844\n",
      "chairman: 63.91004943847656\n",
      "director: 61.21501922607422\n",
      "house: 59.91638946533203\n",
      "president: 56.692054748535156\n",
      "james: 56.09077835083008\n",
      "representative: 52.293128967285156\n",
      "ministry: 51.10329055786133\n",
      "department: 50.85832214355469\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Load the saved word vectors\n",
    "word_vectors = KeyedVectors.load(\"word_vectors.kv\")\n",
    "\n",
    "query_word = \"secretary\"  # Example query word\n",
    "\n",
    "# Compute the top 10 most similar words to 'king'\n",
    "top_similar_words = compute_dot_product(query_word, word_vectors, top_n=10)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Top 10 most similar words to '{query_word}':\")\n",
    "for word, score in top_similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
