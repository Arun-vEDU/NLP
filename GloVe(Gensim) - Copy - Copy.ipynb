{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Arunya\n",
      "[nltk_data]     Senadeera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the reuters dataset if not already present\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus from the NLTK Reuters dataset\n",
    "def preprocess_reuters():\n",
    "    \"\"\"\n",
    "    Preprocess the Reuters dataset into tokenized sentences for training GloVe.\n",
    "    Returns:\n",
    "        sentences (list): A list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = nltk.word_tokenize(reuters.raw(file_id).lower())\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train GloVe using Word2Vec (CBOW-based approximation)\n",
    "def train_glove_model(sentences, vector_size=100, window_size=2, min_count=5, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a GloVe-like model using Word2Vec from Gensim.\n",
    "    Parameters:\n",
    "        sentences (list): Tokenized sentences from the corpus.\n",
    "        vector_size (int): Dimensionality of the word embeddings.\n",
    "        window_size (int): The context window size (default: 2).\n",
    "        min_count (int): Minimum word frequency to include in the vocabulary.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        model (Word2Vec): The trained Word2Vec model.\n",
    "        training_loss (list): List of losses after each epoch.\n",
    "        training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        vector_size=vector_size,\n",
    "        window=window_size,\n",
    "        min_count=min_count,\n",
    "        sg=0,  # CBOW model (default behavior of GloVe)\n",
    "        compute_loss=True  # Enable loss computation\n",
    "    )\n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    training_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "        training_loss.append(loss)\n",
    "\n",
    "    # Record the end time\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return model, training_loss, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Reuters dataset\n",
    "sentences = preprocess_reuters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/10...\n",
      "Epoch 1, Loss: 0.0\n",
      "Training epoch 2/10...\n",
      "Epoch 2, Loss: 0.0\n",
      "Training epoch 3/10...\n",
      "Epoch 3, Loss: 0.0\n",
      "Training epoch 4/10...\n",
      "Epoch 4, Loss: 0.0\n",
      "Training epoch 5/10...\n",
      "Epoch 5, Loss: 0.0\n",
      "Training epoch 6/10...\n",
      "Epoch 6, Loss: 0.0\n",
      "Training epoch 7/10...\n",
      "Epoch 7, Loss: 0.0\n",
      "Training epoch 8/10...\n",
      "Epoch 8, Loss: 0.0\n",
      "Training epoch 9/10...\n",
      "Epoch 9, Loss: 0.0\n",
      "Training epoch 10/10...\n",
      "Epoch 10, Loss: 0.0\n",
      "Total Training Time: 4.98 seconds\n",
      "Training Loss Per Epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Train the GloVe-like model\n",
    "glove_model, training_loss, training_time = train_glove_model(sentences, vector_size=100, window_size=2)\n",
    "\n",
    "# Print training summary\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Training Loss Per Epoch: {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 nouns in the corpus:\n",
      "[('The', 10967), ('U.S.', 4919), ('Net', 3378), ('Shr', 3201), ('Inc', 2663), ('Corp', 2336), ('Revs', 2283), ('It', 2059), ('April', 1902), ('March', 1817), ('Bank', 1704), ('He', 1613), ('February', 1574), ('Japan', 1555), ('January', 1549), ('In', 1474), ('Co', 1387), ('A', 1276), ('Ltd', 1179), ('But', 1157), ('Avg', 1062), ('Oper', 1048), ('May', 964), ('Japanese', 890), ('December', 881), ('Sales', 873), ('West', 858), ('United', 835), ('International', 822), ('New', 795), ('American', 792), ('I', 703), ('We', 688), ('June', 670), ('Group', 653), ('States', 640), ('European', 625), ('Commission', 625), ('Department', 622), ('Year', 620), ('This', 593), ('U.S', 579), ('They', 562), ('Federal', 555), ('National', 549), ('Canada', 546), ('Canadian', 537), ('Nine', 527), ('Minister', 525), ('U.K.', 522)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Extract and preprocess the corpus\n",
    "def get_corpus():\n",
    "    \"\"\"\n",
    "    Extract tokenized words from the Reuters corpus and preprocess them.\n",
    "    Returns:\n",
    "        tokenized_words (list): List of all words in the corpus.\n",
    "    \"\"\"\n",
    "    tokenized_words = []\n",
    "    for file_id in reuters.fileids():\n",
    "        words = word_tokenize(reuters.raw(file_id))\n",
    "        tokenized_words.extend(words)\n",
    "    return tokenized_words\n",
    "\n",
    "# Identify nouns in the corpus\n",
    "def identify_nouns(words):\n",
    "    \"\"\"\n",
    "    Identify proper nouns and common nouns from the word list.\n",
    "    Parameters:\n",
    "        words (list): List of words.\n",
    "    Returns:\n",
    "        nouns (list): List of potential nouns (words starting with uppercase).\n",
    "    \"\"\"\n",
    "    nouns = [word for word in words if word.istitle()]  # Filter capitalized words\n",
    "    return nouns\n",
    "\n",
    "# Get the corpus\n",
    "corpus_words = get_corpus()\n",
    "\n",
    "# Count and print a sample of nouns\n",
    "nouns = identify_nouns(corpus_words)\n",
    "nouns_count = Counter(nouns)\n",
    "\n",
    "# Print the top 50 most common nouns\n",
    "print(\"Top 50 nouns in the corpus:\")\n",
    "print(nouns_count.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'Canada': [-0.50394714 -0.907235   -0.32813582  0.6879264  -0.7342884   0.75093186\n",
      "  0.61239594  0.34096304 -0.31239104 -1.5388498  -0.32393542 -1.669975\n",
      "  0.8280759  -0.11881369  0.00401857  0.7257953  -0.11328927 -0.0295985\n",
      " -0.27693912 -0.5178133   0.64596087 -1.528738   -0.46207717  0.719274\n",
      "  1.6206697  -0.56574494 -1.6323107  -0.45002523  0.50140965 -0.7647967\n",
      " -0.5006929  -0.5909827   0.28333822 -0.5740105  -0.26148933 -0.58785313\n",
      " -0.14032222 -0.26196045 -0.13800836  0.6306168   1.0127885  -0.34454647\n",
      " -0.02447197  0.11158471 -0.9486377   0.7733709  -0.20020643  0.8107985\n",
      "  0.9236379   0.563618   -0.25160363 -0.24732193  0.35184932 -1.4933707\n",
      " -0.91247404  0.18238029  0.59087384  0.20345269  1.2826673   1.4506941\n",
      "  0.60126007 -0.81656635 -0.36236653 -0.4306596   0.24310616  1.3210568\n",
      " -0.62460846  0.07269536 -0.95956945 -0.5705752  -0.39182952 -0.6005903\n",
      "  0.56646276 -0.6529269   1.2410403  -0.15492062 -0.08251475 -0.58457595\n",
      " -0.47248563 -0.36001197 -0.9090241   0.7020806  -1.5352061  -0.69068676\n",
      " -0.11168368  0.55422676  0.9511992  -0.6076023  -0.5728617  -0.4974499\n",
      "  1.8095032   0.10348359 -0.6157484   0.80278236 -0.08638733 -0.23560362\n",
      "  0.29361355  1.1299044   0.3221796   0.4468211 ]\n",
      "Most similar to 'Minister': [('commissioner', 0.6733689308166504), ('kiichi', 0.6654211282730103), ('edouard', 0.6610682606697083), ('undersecretary', 0.6601791381835938), ('yasuhiro', 0.6581844091415405), ('manuel', 0.6518183350563049), ('margaret', 0.6411091089248657), ('gerhard', 0.6401329040527344), ('sheikh', 0.6366317272186279), ('jacques', 0.631130039691925)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the trained model\n",
    "word_vectors = glove_model.wv # holds the trained word vectors (embeddings) learned by the Word2Vec model\n",
    "print(\"Vector for 'Canada':\", word_vectors['canada'])\n",
    "print(\"Most similar to 'Minister':\", word_vectors.most_similar('minister'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the word_vectors from  trained model\n",
    "word_vectors = glove_model.wv  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import requests\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "def load_wordsim353(filepath):\n",
    "    word_pairs = []\n",
    "    human_scores = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            word1, word2, score = row\n",
    "            word_pairs.append((word1, word2))\n",
    "            human_scores.append(float(score))\n",
    "    return word_pairs, human_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Calculate model similarity scores using dot product\n",
    "def calculate_model_scores(word_pairs, model):\n",
    "    model_scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in model.key_to_index and word2 in model.key_to_index:\n",
    "            vector1 = model[word1]\n",
    "            vector2 = model[word2]\n",
    "            dot_product = np.dot(vector1, vector2)\n",
    "            model_scores.append(dot_product)\n",
    "        else:\n",
    "            # Assign a low score if one or both words are missing\n",
    "            model_scores.append(0.0)\n",
    "    return model_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: 0.0066\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Evaluate Spearman correlation\n",
    "def evaluate_correlation(human_scores, model_scores):\n",
    "    correlation, _ = spearmanr(human_scores, model_scores)\n",
    "    return correlation\n",
    "\n",
    "# Main Code\n",
    "\n",
    "word_pairs, human_scores = load_wordsim353(\"wordsim_similarity_goldstandard.txt\")\n",
    "\n",
    "# Use the  trained  GloVe model\n",
    "word_vectors = glove_model.wv \n",
    "model_scores = calculate_model_scores(word_pairs, word_vectors)\n",
    "correlation = evaluate_correlation(human_scores, model_scores)\n",
    "\n",
    "\n",
    "print(f\"Spearman Correlation: {correlation:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
